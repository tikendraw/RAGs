{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv(\"../.env\")\n",
    "\n",
    "\n",
    "os.environ[\"HF_HOME\"] = \"/home/t/.cache/huggingface\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nest_asyncio import apply\n",
    "\n",
    "apply()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/t/atest/llamaindex-agents/.venv/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from llama_index.embeddings.huggingface import HuggingFaceEmbedding\n",
    "from llama_index.core.postprocessor import SentenceTransformerRerank\n",
    "from llama_index.core.schema import NodeWithScore, Node"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# rerank = SentenceTransformerRerank(\n",
    "#     model=\"cross-encoder/ms-marco-MiniLM-L-2-v2\", top_n=3\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core.workflow import Event\n",
    "from llama_index.core.schema import NodeWithScore, Node\n",
    "\n",
    "\n",
    "class RetrieverEvent(Event):\n",
    "    \"\"\"Result of running retrieval\"\"\"\n",
    "\n",
    "    nodes: list[NodeWithScore]\n",
    "\n",
    "\n",
    "class RerankEvent(Event):\n",
    "    \"\"\"Result of running reranking on retrieved nodes\"\"\"\n",
    "\n",
    "    nodes: list[NodeWithScore]\n",
    "\n",
    "\n",
    "class SearchEvent(Event):\n",
    "    query: str"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from llama_index.embeddings.huggingface import HuggingFaceEmbedding\n",
    "\n",
    "# # loads BAAI/bge-small-en\n",
    "# # embed_model = HuggingFaceEmbedding()\n",
    "\n",
    "# # loads BAAI/bge-small-en-v1.5\n",
    "# embed_model = HuggingFaceEmbedding(model_name=\"BAAI/bge-small-en-v1.5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core import SimpleDirectoryReader, VectorStoreIndex\n",
    "from llama_index.core.response_synthesizers import CompactAndRefine\n",
    "from llama_index.core.postprocessor.llm_rerank import LLMRerank\n",
    "from llama_index.core.workflow import (\n",
    "    Context,\n",
    "    Workflow,\n",
    "    StartEvent,\n",
    "    StopEvent,\n",
    "    step,\n",
    ")\n",
    "\n",
    "from llama_index.core.ingestion import IngestionPipeline, IngestionCache\n",
    "from llama_index.core.node_parser import (\n",
    "    SentenceSplitter,\n",
    "    TextSplitter,\n",
    "    HierarchicalNodeParser,\n",
    ")\n",
    "from llama_index.vector_stores.qdrant import QdrantVectorStore\n",
    "\n",
    "import qdrant_client\n",
    "from llama_index.core import VectorStoreIndex\n",
    "from llama_index.core.workflow import Workflow, step, StopEvent, StartEvent, Context\n",
    "from llama_index.core.node_parser import get_leaf_nodes, get_root_nodes\n",
    "from llama_index.core import StorageContext\n",
    "from llama_index.core import VectorStoreIndex\n",
    "from llama_index.core.retrievers import AutoMergingRetriever\n",
    "\n",
    "from llama_index.core.indices import vector_store\n",
    "\n",
    "\n",
    "# from llama_index.llms.groq import Groq\n",
    "from llama_index.llms.gemini import Gemini\n",
    "from llama_index.llms.ollama import Ollama\n",
    "from llama_index.embeddings.gemini import GeminiEmbedding\n",
    "from llama_index.embeddings.ollama import OllamaEmbedding\n",
    "from llama_index.core import Settings\n",
    "from llama_index.core.data_structs import Node\n",
    "from llama_index.core.response_synthesizers import ResponseMode\n",
    "from llama_index.core import get_response_synthesizer\n",
    "from llama_index.embeddings.huggingface import HuggingFaceEmbedding\n",
    "from pprint import pprint\n",
    "from llama_index.core.postprocessor import SentenceTransformerRerank\n",
    "from llama_index.core.storage.docstore import SimpleDocumentStore"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model setting\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_node(node: Node | NodeWithScore, metadata: bool = True):\n",
    "    char = 300\n",
    "    print(\"id: \", node.id_)\n",
    "    print(\"=\" * 10, \"METADATA\", \"=\" * 10)\n",
    "    pprint(node.metadata, indent=4)\n",
    "    print(\"=\" * 10, \"TEXT\", \"=\" * 10)\n",
    "    print(node.text[:char] + f\"...({len(node.text[char:])} chars truncated.)\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.llms.groq import Groq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "groq_llm = Groq(model='llama-3.1-8b-instant')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# llm = Gemini(model=\"models/gemini-1.5-pro\")\n",
    "ollama_llm = Ollama(model=\"qwen2:0.5b\")\n",
    "\n",
    "# embedding_model = GeminiEmbedding()\n",
    "embedding_model2 = HuggingFaceEmbedding(\n",
    "    model_name=\"BAAI/bge-small-en-v1.5\", cache_folder=os.environ[\"HF_HOME\"] + \"/hub\"\n",
    ")\n",
    "embedding_model3 = OllamaEmbedding(model_name=\"all-minilm\")\n",
    "\n",
    "Settings.llm = ollama_llm\n",
    "Settings.embed_model = embedding_model3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Settings.llm = llm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_retriever(dirname: str):\n",
    "    client = qdrant_client.QdrantClient(location=\":memory:\")\n",
    "    vector_store = QdrantVectorStore(client=client, collection_name=\"test_store\")\n",
    "    # vector_store = VectorStore()\n",
    "\n",
    "    transformations = [\n",
    "        # SentenceSplitter(separator=\"\\n\", chunk_overlap=30, chunk_size=150),\n",
    "        HierarchicalNodeParser.from_defaults()\n",
    "    ]\n",
    "\n",
    "    embed_model = OllamaEmbedding(model_name=\"all-minilm\")\n",
    "\n",
    "    transformations = transformations + [embed_model]\n",
    "\n",
    "    ingestion_pipeline = IngestionPipeline(transformations=transformations)\n",
    "\n",
    "    documents = SimpleDirectoryReader(dirname).load_data()\n",
    "    nodes = ingestion_pipeline.run(documents=documents)\n",
    "    leaf_nodes = get_leaf_nodes(nodes)\n",
    "    docstore = SimpleDocumentStore()\n",
    "\n",
    "    # insert nodes into docstore\n",
    "    docstore.add_documents(nodes)\n",
    "\n",
    "    # define storage context (will include vector store by default too)\n",
    "    storage_context = StorageContext.from_defaults(docstore=docstore)\n",
    "    # vector_store.add(nodes)\n",
    "    # storage_context = StorageContext.from_defaults(vector_store=vector_store)\n",
    "    base_index = VectorStoreIndex(leaf_nodes, storage_context=storage_context)\n",
    "    base_retriever = base_index.as_retriever(similarity_top_k=3)\n",
    "    retriever = AutoMergingRetriever(base_retriever, storage_context, verbose=True)\n",
    "\n",
    "    return base_retriever, retriever"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# base_retriever, retriever = get_retriever(\"data\")\n",
    "# print(retriever)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# nodes = retriever.retrieve(\"spatial\")\n",
    "# base_nodes = base_retriever.retrieve(\"spatial\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(\"nodes:\")\n",
    "# print(nodes)\n",
    "# print()\n",
    "# print(\"Base Nodes: \")\n",
    "# print(base_nodes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# len(nodes), len(base_nodes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from llama_index.core.response.notebook_utils import display_source_node\n",
    "\n",
    "# for node in nodes:\n",
    "#     display_source_node(node, source_length=10000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for node in base_nodes:\n",
    "#     display_source_node(node, source_length=10000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print_node(nodes[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def node_text(node):\n",
    "    print(\"id: \", node.id_)\n",
    "    print(\"Similarity: \", node.score)\n",
    "    text = (node.text).strip()\n",
    "    char = 300\n",
    "    rest_char_len = len(text[char:])\n",
    "    print(\n",
    "        \"Text: \",\n",
    "        (\n",
    "            text[:char] + f\"...({rest_char_len} chars truncated.)\"\n",
    "            if rest_char_len\n",
    "            else \" \"\n",
    "        ),\n",
    "    )\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for node in nodes:\n",
    "#     node_text(node)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# nodes[0].text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for node in base_nodes:\n",
    "#     node_text(node)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Rag development\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataclasses import dataclass\n",
    "\n",
    "\n",
    "class ReflectionEvent(Event):\n",
    "    \"\"\"Result of Reflection on the model's generated answer\"\"\"\n",
    "\n",
    "    answer: str\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class ReflectionOutput:\n",
    "    requires_regeneration: bool\n",
    "    reason: str\n",
    "\n",
    "\n",
    "class ReDoEvent(Event):\n",
    "    \"\"\"Result after Reflection to again generate the answer\"\"\"\n",
    "\n",
    "    query: str\n",
    "    old_answer: str\n",
    "    reason: str"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'requires_regeneration': True, 'reason': 'no reason'}"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "aa = json.loads('{\"requires_regeneration\":true, \"reason\":\"no reason\"}')\n",
    "aa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# aa = ReflectionOutput('{\"requires_regeneration\":\"True\", \"reason\":\"no reason\"}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# aa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core import PromptTemplate\n",
    "import time\n",
    "import re\n",
    "from llama_index.utils.workflow import (\n",
    "    draw_all_possible_flows,\n",
    "    draw_most_recent_execution,\n",
    ")\n",
    "from llama_index.core.base.llms.types import ChatMessage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def time_it(func):\n",
    "    \"\"\"\n",
    "    This decorator measures the execution time of a function.\n",
    "    \"\"\"\n",
    "    import time\n",
    "\n",
    "    def wrapper(*args, **kwargs):\n",
    "        start_time = time.time()\n",
    "        result = func(*args, **kwargs)\n",
    "        end_time = time.time()\n",
    "        print(\n",
    "            f\"Function '{func.__name__}' took {end_time - start_time:.4f} seconds to execute.\"\n",
    "        )\n",
    "        return result\n",
    "\n",
    "    return wrapper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_info_from_json_string(json_string):\n",
    "    # Define a regex pattern to extract \"requires_regeneration\" and \"reason\"\n",
    "    pattern = re.compile(\n",
    "        r'\\{\\s*\"requires_regeneration\":\\s*(True|False)\\s*,\\s*\"reason\":\\s*\"([^\"]*)\"\\s*\\}',\n",
    "        re.DOTALL,\n",
    "    )\n",
    "\n",
    "    # Search for the pattern in the JSON string\n",
    "    match = pattern.search(json_string)\n",
    "\n",
    "    if match:\n",
    "        requires_regeneration = match.group(1)\n",
    "        reason = match.group(2)\n",
    "        return {\n",
    "            \"requires_regeneration\": requires_regeneration == \"True\",\n",
    "            \"reason\": reason,\n",
    "        }\n",
    "    else:\n",
    "        # Handle cases where the regex does not match the expected format\n",
    "        return {\n",
    "            \"requires_regeneration\": False,\n",
    "            \"reason\": \"Unable to parse the provided JSON string.\",\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "# llm = ollama_llm\n",
    "llm = Gemini(model=\"models/gemini-1.5-pro\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "from json import JSONDecodeError\n",
    "import timeit\n",
    "\n",
    "from llama_index.core.prompts import ChatMessage\n",
    "from llama_index.core.storage import docstore\n",
    "\n",
    "\n",
    "class RAGWorkflow(Workflow):\n",
    "\n",
    "    def __init__(self, llm, *args, **kwargs):\n",
    "        super().__init__(*args, **kwargs)\n",
    "\n",
    "        # self.ranker = SentenceTransformerRerank(\n",
    "        #     model=\"cross-encoder/ms-marco-MiniLM-L-2-v2\", top_n=3\n",
    "        # )\n",
    "        self.client = None\n",
    "        self.vector_store = None\n",
    "        self.llm = llm\n",
    "        self.llm2 = llm\n",
    "        self.docstore = None\n",
    "        self.index = None\n",
    "        self.storage_context = None\n",
    "        self.nodes = []\n",
    "        self.setup()\n",
    "        self.redo = 0\n",
    "        self.redo_limit = 3\n",
    "        self.history = [ChatMessage(role='system', content=\"you are a helpful assistant.\")]\n",
    "\n",
    "    def setup(self):\n",
    "        self.client = qdrant_client.QdrantClient(location=\":memory:\")\n",
    "        self.vector_store = QdrantVectorStore(\n",
    "            client=self.client, collection_name=\"test_store\"\n",
    "        )\n",
    "\n",
    "    @step(pass_context=True)\n",
    "    async def ingest(self, ctx: Context, ev: StartEvent) -> StopEvent | None:\n",
    "        \"\"\"Entry point to ingest a document, triggered by a StartEvent with `dirname`.\"\"\"\n",
    "        start_time = time.perf_counter()\n",
    "        dirname = ev.get(\"dirname\")\n",
    "        transformations = ev.get(\"transformations\")\n",
    "        embed_model = ev.get(\"embed_model\")\n",
    "\n",
    "        if not dirname:\n",
    "            return None\n",
    "\n",
    "        # vector_store = VectorStore()\n",
    "        if not transformations:\n",
    "            transformations = [\n",
    "                SentenceSplitter(separator=\"\\n\", chunk_overlap=30, chunk_size=150),\n",
    "                HierarchicalNodeParser.from_defaults(),\n",
    "            ]\n",
    "\n",
    "        if not embed_model:\n",
    "            embed_model = OllamaEmbedding(model_name=\"all-minilm\")\n",
    "\n",
    "        transformations = transformations + [embed_model]\n",
    "\n",
    "        ingestion_pipeline = IngestionPipeline(transformations=transformations)\n",
    "\n",
    "        documents = SimpleDirectoryReader(dirname).load_data()\n",
    "        nodes = ingestion_pipeline.run(documents=documents)\n",
    "        leaf_nodes = get_leaf_nodes(nodes)\n",
    "\n",
    "        # insert nodes into docstore\n",
    "        if not self.docstore:\n",
    "            self.docstore = SimpleDocumentStore()\n",
    "\n",
    "        self.docstore.add_documents(nodes)\n",
    "\n",
    "        self.storage_context = StorageContext.from_defaults(docstore=self.docstore)\n",
    "\n",
    "        if not self.index:\n",
    "            self.index = VectorStoreIndex(\n",
    "                leaf_nodes, show_progress=True, storage_context=self.storage_context\n",
    "            )\n",
    "        else:\n",
    "            self.index.insert_nodes(leaf_nodes)\n",
    "\n",
    "        ctx.data[\"index\"] = self.index\n",
    "        ctx.data[\"ingestion_pipeline\"] = ingestion_pipeline\n",
    "        ctx.data[\"vector_store\"] = self.vector_store\n",
    "        ctx.data[\"storage_context\"] = self.storage_context\n",
    "\n",
    "        print(f\"Ingestion Took {time.perf_counter()-start_time} units\")\n",
    "        return StopEvent(result=f\"Indexed {len(documents)} documents.\")\n",
    "\n",
    "    def clean_db(self):\n",
    "        self.setup()\n",
    "        self.index = None\n",
    "        self.docstore = None\n",
    "\n",
    "    @step(pass_context=True)\n",
    "    async def retrieve(self, ctx: Context, ev: StartEvent) -> RerankEvent | None:\n",
    "        \"Entry point for RAG, triggered by a StartEvent with `query`.\"\n",
    "        start_time = time.perf_counter()\n",
    "\n",
    "        query = ev.get(\"query\")\n",
    "        if not query:\n",
    "            return None\n",
    "\n",
    "        print(f\"Query the database with: {query}\")\n",
    "\n",
    "        # store the query in the global context\n",
    "        ctx.data[\"query\"] = query\n",
    "\n",
    "        # get the index from the global context\n",
    "        index = ctx.data.get(\"index\")\n",
    "        if index is None:\n",
    "            print(\"Index is empty, load some documents before querying!\")\n",
    "            return None\n",
    "\n",
    "        base_retriever = index.as_retriever(similarity_top_k=3)\n",
    "        retriever = AutoMergingRetriever(\n",
    "            base_retriever, ctx.data[\"storage_context\"], verbose=True\n",
    "        )\n",
    "\n",
    "        nodes = retriever.retrieve(query)\n",
    "        print(f\"Retrieved {len(nodes)} nodes.\")\n",
    "        print(f\"Retrieve Took {time.perf_counter()-start_time} units\")\n",
    "\n",
    "        return RerankEvent(nodes=nodes)\n",
    "\n",
    "    # @step(pass_context=True)\n",
    "    # async def rerank(self, ctx: Context, ev: RetrieverEvent) -> RerankEvent:\n",
    "    #     start_time = time.perf_counter()\n",
    "\n",
    "    #     print(\"Reranking...\")\n",
    "    #     print(ctx.data.get(\"query\"), flush=True)\n",
    "    #     new_nodes = self.ranker.postprocess_nodes(\n",
    "    #         ev.nodes, query_str=ctx.data.get(\"query\")\n",
    "    #     )\n",
    "    #     print(f\"Reranked nodes to {len(new_nodes)}\")\n",
    "    #     print(f\"Rerank Took {time.perf_counter()-start_time} units\")\n",
    "\n",
    "    #     return RerankEvent(nodes=new_nodes)\n",
    "\n",
    "    @step(pass_context=True)\n",
    "    async def synthesize(self, ctx: Context, ev: RerankEvent) -> ReflectionEvent:\n",
    "        \"\"\"Return a streaming response using reranked nodes.\"\"\"\n",
    "        start_time = time.perf_counter()\n",
    "\n",
    "        query = ctx.data.get(\"query\")\n",
    "        context = \"\\n\\n\".join(node.text for node in ev.nodes)\n",
    "        ctx.data[\"context\"] = context\n",
    "        qa_prompt_tmpl_str = (\n",
    "            \"Context information is below.\\n\"\n",
    "            \"---------------------\\n\"\n",
    "            \"{context_str}\\n\"\n",
    "            \"---------------------\\n\"\n",
    "            \"Given the context information and not prior knowledge, \"\n",
    "            \"follow any given instructions in query.\"\n",
    "            \"answer the query.\\n\"\n",
    "            \"Query: {query_str}\\n\"\n",
    "            \"Answer: \"\n",
    "        )\n",
    "        qa_prompt_tmpl = PromptTemplate(qa_prompt_tmpl_str)\n",
    "        prompt = qa_prompt_tmpl.format(query_str=query, context_str=context)\n",
    "        chatmessage = ChatMessage(role='user', content=prompt)\n",
    "        response = self.llm.chat(messages=self.history+[chatmessage])\n",
    "        self.history.append(response.message)\n",
    "        print(\"type: \", type(response))\n",
    "        print(f\"Synthesize Took {time.perf_counter()-start_time} units\")\n",
    "        return ReflectionEvent(answer=str(response.message.content))\n",
    "\n",
    "    @step(pass_context=True)\n",
    "    async def resynthesize(self, ctx: Context, ev: ReDoEvent) -> ReflectionEvent:\n",
    "        \"\"\"Return a streaming response using reranked nodes.\"\"\"\n",
    "        start_time = time.perf_counter()\n",
    "\n",
    "        query = ctx.data.get(\"query\")\n",
    "        context = ctx.data[\"context\"]\n",
    "\n",
    "        qa_prompt_tmpl_str = (\n",
    "            \"Context information is below.\\n\"\n",
    "            \"---------------------\\n\"\n",
    "            \"{context_str}\\n\"\n",
    "            \"---------------------\\n\"\n",
    "            \"Given the context information and not prior knowledge, \"\n",
    "            \"follow any given instructions in query.\"\n",
    "            \"answer the query.\\n\"\n",
    "            \"Query: {query_str}\\n\"\n",
    "            \"you already have generated these answers \\n\"\n",
    "            \"Previous answers: {previous_answer}\\n\"\n",
    "            \"Previous answers was rejected for the reason: {reason}\\n\"\n",
    "            \"now try to answer it again.\\n\"\n",
    "            \"Answer: \"\n",
    "        )\n",
    "        qa_prompt_tmpl = PromptTemplate(qa_prompt_tmpl_str)\n",
    "        prompt = qa_prompt_tmpl.format(\n",
    "            query_str=query,\n",
    "            context_str=context,\n",
    "            previous_answer=ev.old_answer,\n",
    "            reason=ev.reason,\n",
    "        )\n",
    "        response = await self.llm.acomplete(prompt)\n",
    "\n",
    "        print(f\"RESynthesize Took {time.perf_counter()-start_time} units\")\n",
    "        return ReflectionEvent(answer=response.text)\n",
    "\n",
    "    @step(pass_context=True)\n",
    "    async def reflection(\n",
    "        self, ctx: Context, ev: ReflectionEvent\n",
    "    ) -> StopEvent | ReDoEvent:\n",
    "        ans = ev.answer\n",
    "        query = ctx.data[\"query\"]\n",
    "\n",
    "        reflection_prompt = \"\"\"For the given question : \\n\n",
    "        \n",
    "        question: \"{query}\" \\n  \n",
    "        generated answer: \"{answer}\" \\n\n",
    "        \n",
    "        does the generated answer, answers the question satisfactorily,\n",
    "        if the generated answer is not answering the question then return a json (with requires_regeneration:bool and reason:str key)\n",
    "        with keys as requires_regeneration:bool (True if answer is not statisfying to the query and false if it is a good answer)\n",
    "        another key is reason:str (reason for the regeneration if regeneration is required else empty string)\n",
    "\n",
    "        so if the answer is not helpful then return this:\n",
    "        \n",
    "        \"requires_regeneration\":True, \"reason\": \" some reason explaining the requirement of regeneration\"\n",
    "        \n",
    "        or if the answer is helpful then set requires_regeneration to False with empty reason\n",
    "\n",
    "       \"requires_regeneration\":False, \"reason\":\" \"\n",
    "       \n",
    "       it should be parsable dict to class\n",
    "       @dataclass\n",
    "       class ReflectionOutput:\n",
    "            requires_regeneration:bool\n",
    "            reason:str\n",
    "        \n",
    "        output should be a valid json        \n",
    "        \"\"\"\n",
    "        reflection_tmpl = PromptTemplate(reflection_prompt)\n",
    "\n",
    "        prompt = reflection_tmpl.format(query=query, answer=ans)\n",
    "        response = await self.llm2.acomplete(prompt)\n",
    "        try:\n",
    "            response = json.loads(response.text)\n",
    "        except JSONDecodeError:\n",
    "            response = extract_info_from_json_string(response.text)\n",
    "\n",
    "        if response.get(\"requires_regeneration\") and self.redo <= self.redo_limit:\n",
    "            return ReDoEvent(query=query, old_answer=ans, reason=response.get(\"reason\"))\n",
    "        else:\n",
    "\n",
    "            return StopEvent(result=ans)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "w = RAGWorkflow(llm=groq_llm, timeout=60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "# a = groq_llm.chat(messages=[\n",
    "#     ChatMessage(role='system', content=\"you are a helpful assistant\"),\n",
    "#     ChatMessage(role='user',content=\"how old is sun, just give me the number\")\n",
    "#     ])\n",
    "# a, type(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "# draw_all_possible_flows(w)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating embeddings: 0it [00:00, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ingestion Took 5.3032840879996 units\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'Indexed 2 documents.'"
      ]
     },
     "execution_count": 119,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Ingest the documents\n",
    "await w.run(dirname=\"data2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "# w.clean_db()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "# await w.run(dirname=\"data3\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query the database with: how to contact tikendra to tell he is selected for the job\n",
      "> Merging 2 nodes into parent node.\n",
      "> Parent node id: 9fedb3be-9b90-43b1-9385-e4fbdf11311f.\n",
      "> Parent node text: .\n",
      "Coursera, GOOGLE•2022\n",
      "   https://coursera.org/verify/829MQ3JFKEAQ •\n",
      "DeepLearning.AI : Neural Ne...\n",
      "\n",
      "> Merging 1 nodes into parent node.\n",
      "> Parent node id: 4a41279e-2b37-4dd6-bc75-2fe3fb24a634.\n",
      "> Parent node text: .\n",
      "Coursera, GOOGLE•2022\n",
      "   https://coursera.org/verify/829MQ3JFKEAQ •\n",
      "DeepLearning.AI : Neural Ne...\n",
      "\n",
      "Retrieved 2 nodes.\n",
      "Retrieve Took 0.1168908729996474 units\n",
      "type:  <class 'llama_index.core.base.llms.types.ChatResponse'>\n",
      "Synthesize Took 1.043699363000087 units\n",
      "Based on the provided context information, I can suggest a way to contact Tikendra Kumar Sahu.\n",
      "\n",
      "You can contact Tikendra Kumar Sahu through the following methods:\n",
      "\n",
      "1. Email: tikendraworks@gmail.com\n",
      "2. Phone: 9926134994\n",
      "3. LinkedIn: in/tikendraw (Note: You can send him a LinkedIn message or request to connect)\n",
      "\n",
      "To inform him that he is selected for the job, you can draft a message like this:\n",
      "\n",
      "\"Dear Tikendra,\n",
      "\n",
      "I am pleased to inform you that you have been selected for the position. We believe that your skills and experience make you an ideal fit for our team. We would like to discuss the details of your selection and the next steps.\n",
      "\n",
      "Please let us know a convenient time to schedule a call to discuss further.\n",
      "\n",
      "Best regards, [Your Name]\"\n",
      "\n",
      "You can modify the message according to your needs and send it to him through the chosen contact method.\n"
     ]
    }
   ],
   "source": [
    "# Run a query\n",
    "result = await w.run(query=\"how to contact tikendra to tell him he is selected for the job\")\n",
    "# async for chunk in result.async_response_gen():\n",
    "#     print(chunk, end=\"\", flush=True)\n",
    "print(str(result))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CompletionResponse(text=\"**Related Work in Video Understanding**\\n\\nThe provided text focuses on **spatiotemporal features for video analysis** as a key aspect of video understanding research. \\n\\nHere's a breakdown:\\n\\n* **Spatiotemporal Features:** These features capture information about both **spatial** (location within a frame) and **temporal** (changes over time) aspects of a video. They are crucial for understanding actions, events, and relationships within videos.\\n\\n* **Traditional Approaches:** While not explicitly mentioned, the text alludes to traditional methods using **3D convolutions**. These methods directly process video data as volumetric data, applying convolutional filters in both spatial and temporal dimensions.\\n\\n* **Proposed Approach (Implied):** The text emphasizes the advantages of **decomposing** spatiotemporal convolutions. This suggests the authors are introducing a novel method that separates spatial and temporal processing, potentially leading to:\\n    * **Increased Nonlinearities:**  By introducing an additional rectification step between spatial and temporal operations, the model can learn more complex representations.\\n    * **Parameter Efficiency:** This decomposition might achieve comparable or better performance with fewer parameters than full 3D convolutions.\\n\\n* **Visualization of Temporal Filters:** The text mentions visualizing temporal filters as matrices. This suggests the authors analyze and interpret learned temporal patterns to gain insights into how their model processes temporal information.\\n\\n**In summary:** The related work focuses on the evolution of techniques for extracting spatiotemporal features in video understanding, highlighting the limitations of traditional 3D convolutions and hinting at the benefits of the authors' proposed decomposition approach. \\n\", additional_kwargs={}, raw={'content': {'parts': [{'text': \"**Related Work in Video Understanding**\\n\\nThe provided text focuses on **spatiotemporal features for video analysis** as a key aspect of video understanding research. \\n\\nHere's a breakdown:\\n\\n* **Spatiotemporal Features:** These features capture information about both **spatial** (location within a frame) and **temporal** (changes over time) aspects of a video. They are crucial for understanding actions, events, and relationships within videos.\\n\\n* **Traditional Approaches:** While not explicitly mentioned, the text alludes to traditional methods using **3D convolutions**. These methods directly process video data as volumetric data, applying convolutional filters in both spatial and temporal dimensions.\\n\\n* **Proposed Approach (Implied):** The text emphasizes the advantages of **decomposing** spatiotemporal convolutions. This suggests the authors are introducing a novel method that separates spatial and temporal processing, potentially leading to:\\n    * **Increased Nonlinearities:**  By introducing an additional rectification step between spatial and temporal operations, the model can learn more complex representations.\\n    * **Parameter Efficiency:** This decomposition might achieve comparable or better performance with fewer parameters than full 3D convolutions.\\n\\n* **Visualization of Temporal Filters:** The text mentions visualizing temporal filters as matrices. This suggests the authors analyze and interpret learned temporal patterns to gain insights into how their model processes temporal information.\\n\\n**In summary:** The related work focuses on the evolution of techniques for extracting spatiotemporal features in video understanding, highlighting the limitations of traditional 3D convolutions and hinting at the benefits of the authors' proposed decomposition approach. \\n\"}], 'role': 'model'}, 'finish_reason': 1, 'index': 0, 'safety_ratings': [], 'token_count': 0, 'grounding_attributions': [], 'block_reason': 0}, logprobs=None, delta=None)"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "workflow_recent_execution.html\n"
     ]
    }
   ],
   "source": [
    "draw_most_recent_execution(w)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core.node_parser import SimpleFileNodeParser\n",
    "from llama_index.readers.file import FlatReader\n",
    "from pathlib import Path\n",
    "\n",
    "dirname = \"./data\"\n",
    "documents = SimpleDirectoryReader(dirname).load_data()\n",
    "index = VectorStoreIndex.from_documents(documents=documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "retriever = index.as_retriever(similarity_top_k=5)\n",
    "nodes = retriever.retrieve(\"what is GAtt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "In ICCV , 2011. 5, 8\n",
      "[21] I. Laptev and T. Lindeberg. Space-time interest points. In\n",
      "ICCV , 2003. 2\n",
      "[22] Q. V . Le, W. Y . Zou, S. Y . Yeung, and A. Y . Ng. Learn-\n",
      "ing hierarchical invariant spatio-temporal features for action\n",
      "recognition with independent subspace analysis. In CVPR ,\n",
      "2011. 2\n",
      "[23] P. Molchanov, X. Yang, S. Gupta, K. Kim, S. Tyree, and\n",
      "J. Kautz. Online detection and classiﬁcation of dynamic hand\n",
      "gestures with recurrent 3d convolutional neural network. In\n",
      "CVPR , 2016. 2\n",
      "[24] Y . Pan, T. Mei, T. Yao, H. Li, and Y . Rui. Jointly modeling\n",
      "embedding and translation to bridge video and language. In\n",
      "CVPR , 2016. 2\n",
      "[25] Z. Qiu, T. Yao, , and T. Mei. Learning spatio-temporal repre-\n",
      "sentation with pseudo-3d residual networks. In ICCV , 2017.\n",
      "1, 2, 4, 7, 8\n",
      "[26] S. Sadanand and J. Corso. Action bank: A high-level repre-\n",
      "sentation of activity in video. In CVPR , 2012. 2\n",
      "[27] P. Scovanner, S. Ali, and M. Shah. A 3-dimensional sift de-\n",
      "scriptor and its application to action recognition. In ACM\n",
      "MM, 2007. 2\n",
      "[28] Z. Shou, D. Wang, and S.-F. Chang. Temporal action local-\n",
      "ization in untrimmed videos via multi-stage cnns. In CVPR ,\n",
      "2016. 2\n",
      "[29] K. Simonyan and A. Zisserman. Two-stream convolutional\n",
      "networks for action recognition in videos. In NIPS , 2014. 2,\n",
      "3, 7, 8\n",
      "*********************************\n",
      "Appendix\n",
      "Figure 6 illustrates the decomposed ﬁlters of our\n",
      "R(2+1)D model at the conv1 layer. We remind the read-\n",
      "ers that, instead of using 643D ﬁlters of size 3×7×7at\n",
      "conv1 as in R3D, R(2+1)D decomposes conv1 into 45\n",
      "2D ﬁlters of size 1×7×7and641D ﬁlters of size 3×1×1\n",
      "with non-linear ReLUs in between. This (2+1)D convolu-\n",
      "tional block has the same number of parameters as that of\n",
      "R3D. Figure 6(a) shows the 45spatial ﬁlters of size 7×7\n",
      "upscaled by 4x for better visualization. Figure 6(b) presents\n",
      "the64temporal ﬁlters from left to right. Each temporal ﬁl-\n",
      "ter is visualized as a 45×3matrix. Each matrix shows how\n",
      "the temporal ﬁlter combines the 45channels from the spa-\n",
      "tial ﬁlters across time ( 3frames). Some interesting temporal\n",
      "patterns can be seen by looking at the ﬁlter weights along\n",
      "the time dimension.\n",
      "References\n",
      "[1] M. Baccouche, F. Mamalet, C. Wolf, C. Garcia, and\n",
      "A. Baskurt. Sequential Deep Learning for Human Ac-\n",
      "tion Recognition , pages 29–39. Springer Berlin Heidelberg,\n",
      "Berlin, Heidelberg, 2011. 2\n",
      "[2] N. Ballas, L. Yao, C. Pal, and A. Courville. Delving deeper\n",
      "into convolutional networks for learning video representa-\n",
      "tions. arXiv preprint arXiv:1511.06432 , 2015. 2\n",
      "[3] Caffe2-Team. Caffe2: A new lightweight, modular, and scal-\n",
      "able deep learning framework. https://caffe2.ai/ . 6\n",
      "[4] J. Carreira and A. Zisserman. Quo vadis, action recognition?\n",
      "a new model and the kinetics dataset. In CVPR , 2017. 1, 3,\n",
      "5, 7, 8\n",
      "[5] N. Dalal, B. Triggs, and C. Schmid. Human Detection\n",
      "Using Oriented Histograms of Flow and Appearance. In\n",
      "A. Leonardis, H. Bischof, and A. Pinz, editors, European\n",
      "Conference on Computer Vision (ECCV ’06) , volume 3952\n",
      "ofLecture Notes in Computer Science (LNCS) , pages 428–\n",
      "441, Graz, Austria, May 2006. Springer-Verlag. 2\n",
      "[6] P. Dollar, V . Rabaud, G. Cottrell, and S. Belongie. Behav-\n",
      "ior recognition via sparse spatio-temporal features. In Proc.\n",
      "ICCV VS-PETS , 2005. 2\n",
      "[7] J. Donahue, L. Anne Hendricks, S. Guadarrama,\n",
      "M. Rohrbach, S. Venugopalan, K. Saenko, and T. Dar-\n",
      "rell. Long-term recurrent convolutional networks for visual\n",
      "recognition and description. In Proceedings of the IEEE\n",
      "conference on computer vision and pattern recognition ,\n",
      "pages 2625–2634, 2015. 2\n",
      "[8] G. Farneb ¨ack. Two-frame motion estimation based on poly-\n",
      "nomial expansion. In Image Analysis, 13th Scandinavian\n",
      "Conference, SCIA 2003, Halmstad, Sweden, June 29 - July\n",
      "2, 2003, Proceedings , pages 363–370, 2003. 7\n",
      "[9] C. Feichtenhofer, A. Pinz, and R. P. Wildes. Spatiotempo-\n",
      "ral residual networks for video action recognition. In NIPS ,\n",
      "2016. 2, 7, 8\n",
      "[10] C. Feichtenhofer, A. Pinz, and A. Zisserman. Convolutional\n",
      "two-stream network fusion for video action recognition. In\n",
      "CVPR , 2016. 2, 8[11] R. Girdhar, D. Ramanan, A. Gupta, J. Sivic, and B. C. Rus-\n",
      "sell.\n",
      "*********************************\n",
      "t x d x d1 x d x d\n",
      "t x 1 x 1Mi\n",
      "a) b)Figure 2. (2+1)D vs 3D convolution . The illustration is given for\n",
      "the simpliﬁed setting where the input consists of a spatiotemporal\n",
      "volume with a single feature channel. (a) Full 3D convolution is\n",
      "carried out using a ﬁlter of size t×d×dwhere tdenotes the tem-\n",
      "poral extent and dis the spatial width and height. (b) A (2+1)D\n",
      "convolutional block splits the computation into a spatial 2D con-\n",
      "volution followed by a temporal 1D convolution. We choose the\n",
      "numbers of 2D ﬁlters ( Mi) so that the number of parameters in our\n",
      "(2+1)D block matches that of the full 3D convolutional block.\n",
      "using 2D convolutions in the top layers. Since in this work\n",
      "we consider 3D ResNets (R3D) having 5groups of convo-\n",
      "lutions (see Table 1), our ﬁrst variant consists in replacing\n",
      "all 3D convolutions in group 5 with 2D convolutions. We\n",
      "denote this variant with MC5 (Mixed Convolutions). We\n",
      "design a second variant that uses 2D convolutions in group\n",
      "4 and 5, and name this model MC4 (meaning from group 4\n",
      "and deeper layers all convolutions are 2D). Following this\n",
      "pattern, we also create MC3 and MC2 variations. We omit\n",
      "to consider MC1 since it is equivalent to a 2D ResNet (f-\n",
      "R2D) applied to clip inputs. This type of CNN architec-\n",
      "tures is illustrated in Figure 1(b). An alternative hypoth-\n",
      "esis is that temporal modeling may be more beneﬁcial in\n",
      "the deep layers, with early capturing appearance informa-\n",
      "tion via 2D convolutions. To account for such possibility,\n",
      "we also experiment with “Reversed” Mixed Convolutions.\n",
      "Following the naming convention of MC models, we de-\n",
      "note these models as rMC2, rMC3, rMC4, and rMC5. Thus,\n",
      "rMC3 would include 2D convolutions in block 1 and 2, and\n",
      "3D convolutions in group 3 and deeper groups. This type of\n",
      "CNN architecture is illustrated in Figure 1(c).\n",
      "3.5. R(2+1)D: (2+1)D convolutions\n",
      "Another possible theory is that full 3D convolutions may\n",
      "be more conveniently approximated by a 2D convolution\n",
      "followed by a 1D convolution, decomposing spatial and\n",
      "temporal modeling into two separate steps. We thus design\n",
      "a network architecture named R(2+1)D, where we replace\n",
      "theNi3D convolutional ﬁlters of size Ni−1×t×d×d\n",
      "with a (2+1)D block consisting of Mi2D convolutional ﬁl-\n",
      "ters of sizeNi−1×1×d×dandNitemporal convolu-\n",
      "tional ﬁlters of size Mi×t×1×1. The hyperparameter\n",
      "Midetermines the dimensionality of the intermediate sub-\n",
      "space where the signal is projected between the spatial and\n",
      "01 02 03 04 05 0\n",
      "epoch00.20.40.60.81error (%)R3D-18 train\n",
      "R3D-18 val\n",
      "R(2+1)D-18 train\n",
      "R(2+1)D-18 val\n",
      "01 02 03 04 05 0\n",
      "epoch00.20.40.60.81error (%)R3D-34 train\n",
      "R3D-34 val\n",
      "R(2+1)D-34 train\n",
      "R(2+1)D-34 valFigure 3. Training and testing errors for R(2+1)D and R3D .\n",
      "Results are reported for ResNets of 18 layers (left) and 34 layers\n",
      "(right). It can be observed that the training error (thin lines) is\n",
      "smaller for R(2+1)D compared to R3D, particularly for the net-\n",
      "work with larger depth (right). This suggests that the the spatial-\n",
      "temporal decomposition implemented by R(2+1)D eases the opti-\n",
      "mization, especially as depth is increased.\n",
      "the temporal convolutions.\n",
      "*********************************\n",
      "Gool. Temporal segment networks: Towards good prac-\n",
      "tices for deep action recognition. In ECCV , 2016. 2, 8\n",
      "[40] X. Wang, A. Farhadi, and A. Gupta. Actions ˜ transforma-\n",
      "tions. In CVPR , 2016. 2, 8\n",
      "[41] Z. Xu, Y . Yang, and A. G. Hauptmann. A discriminative\n",
      "cnn video representation for event detection. In Proceed-\n",
      "ings of the IEEE Conference on Computer Vision and Pattern\n",
      "Recognition , pages 1798–1807, 2015. 2\n",
      "[42] J. Yue-Hei Ng, M. Hausknecht, S. Vijayanarasimhan,\n",
      "O. Vinyals, R. Monga, and G. Toderici. Beyond short snip-pets: Deep networks for video classiﬁcation. In Proceed-\n",
      "ings of the IEEE conference on computer vision and pattern\n",
      "recognition , pages 4694–4702, 2015. 2, 7, 8\n",
      "[43] C. Zach, T. Pock, and H. Bischof. A duality based approach\n",
      "for realtime tv-l 1 optical ﬂow. Pattern Recognition , pages\n",
      "214–223, 2007. 8\n",
      "*********************************\n",
      "A Closer Look at Spatiotemporal Convolutions for Action Recognition\n",
      "Du Tran1, Heng Wang1, Lorenzo Torresani1,2, Jamie Ray1, Yann LeCun1, Manohar Paluri1\n",
      "1Facebook Research2Dartmouth College\n",
      "{trandu,hengwang,torresani,jamieray,yann,mano }@fb.com\n",
      "Abstract\n",
      "In this paper we discuss several forms of spatiotemporal\n",
      "convolutions for video analysis and study their effects on\n",
      "action recognition. Our motivation stems from the observa-\n",
      "tion that 2D CNNs applied to individual frames of the video\n",
      "have remained solid performers in action recognition. In\n",
      "this work we empirically demonstrate the accuracy advan-\n",
      "tages of 3D CNNs over 2D CNNs within the framework of\n",
      "residual learning. Furthermore, we show that factorizing\n",
      "the 3D convolutional ﬁlters into separate spatial and tempo-\n",
      "ral components yields signiﬁcantly gains in accuracy. Our\n",
      "empirical study leads to the design of a new spatiotemporal\n",
      "convolutional block “R(2+1)D” which produces CNNs that\n",
      "achieve results comparable or superior to the state-of-the-\n",
      "art on Sports-1M, Kinetics, UCF101, and HMDB51.\n",
      "1. Introduction\n",
      "Since the introduction of AlexNet [19], deep learning\n",
      "has galvanized the ﬁeld of still-image recognition with a\n",
      "steady sequence of remarkable advances driven by insight-\n",
      "ful design innovations, such as smaller spatial ﬁlters [30],\n",
      "multi-scale convolutions [34], residual learning [13], and\n",
      "dense connections [14]. Conversely, it may be argued that\n",
      "the video domain has not yet witnessed its “AlexNet mo-\n",
      "ment.” While a deep network (I3D [4]) does currently hold\n",
      "the best results in action recognition, the margin of improve-\n",
      "ment over the best hand-crafted approach (iDT [38]) is not\n",
      "as impressive as in the case of image recognition. Further-\n",
      "more, an image-based 2D CNN (ResNet-152 [25]) operat-\n",
      "ing on individual frames of the video achieves performance\n",
      "remarkably close to the state-of-the-art on the challenging\n",
      "Sports-1M benchmark. This result is both surprising and\n",
      "frustrating, given that 2D CNNs are unable to model tem-\n",
      "poral information and motion patterns, which one would\n",
      "deem to be critical aspects for video analysis. Based on\n",
      "such results, one may postulate that temporal reasoning is\n",
      "not essential for accurate action recognition, because of\n",
      "the strong action class information already contained in the\n",
      "static frames of a sequence.In this work, we challenge this view and revisit the role\n",
      "of temporal reasoning in action recognition by means of 3D\n",
      "CNNs, i.e., networks that perform 3D convolutions over the\n",
      "spatiotemporal video volume. While 3D CNNs have been\n",
      "widely explored in the setting of action recognition [15, 35,\n",
      "36, 4], here we reconsider them within the framework of\n",
      "residual learning, which has been shown to be a powerful\n",
      "tool in the ﬁeld of still-image recognition. We demonstrate\n",
      "that 3D ResNets signiﬁcantly outperform 2D ResNets for\n",
      "the same depth when trained and evaluated on large-scale,\n",
      "challenging action recognition benchmarks such as Sports-\n",
      "1M [16] and Kinetics [17].\n",
      "Inspired by these results, we introduce two new forms\n",
      "of spatiotemporal convolution that can be viewed as mid-\n",
      "dle grounds between the extremes of 2D (spatial convolu-\n",
      "tion) and full 3D. The ﬁrst formulation is named mixed con-\n",
      "volution (MC) and consists in employing 3D convolutions\n",
      "only in the early layers of the network, with 2D convolu-\n",
      "tions in the top layers. The rationale behind this design is\n",
      "that motion modeling is a low/mid-level operation that can\n",
      "be implemented via 3D convolutions in the early layers of\n",
      "a network, and spatial reasoning over these mid-level mo-\n",
      "tion features (implemented by 2D convolutions in the top\n",
      "layers) leads to accurate action recognition. We show that\n",
      "MC ResNets yield roughly a 3-4% gain in clip-level ac-\n",
      "curacy over 2D ResNets of comparable capacity and they\n",
      "match the performance of 3D ResNets, which have 3 times\n",
      "as many parameters.\n",
      "*********************************\n"
     ]
    }
   ],
   "source": [
    "for node in nodes:\n",
    "    print(node.text)\n",
    "    print(\"*\" * 33)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Metadata length (9) is close to chunk size (25). Resulting chunks are less than 50 tokens. Consider increasing the chunk size or decreasing the size of your metadata to avoid this.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 20%|██        | 1/5 [00:11<00:47, 11.85s/it]"
     ]
    },
    {
     "ename": "ResourceExhausted",
     "evalue": "429 Resource has been exhausted (e.g. check quota).",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mResourceExhausted\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[34], line 24\u001b[0m\n\u001b[1;32m     13\u001b[0m pipeline \u001b[38;5;241m=\u001b[39m IngestionPipeline(\n\u001b[1;32m     14\u001b[0m     transformations\u001b[38;5;241m=\u001b[39m[\n\u001b[1;32m     15\u001b[0m         SentenceSplitter(chunk_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m25\u001b[39m, chunk_overlap\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     20\u001b[0m     vector_store\u001b[38;5;241m=\u001b[39mvector_store,\n\u001b[1;32m     21\u001b[0m )\n\u001b[1;32m     23\u001b[0m \u001b[38;5;66;03m# Ingest directly into a vector db\u001b[39;00m\n\u001b[0;32m---> 24\u001b[0m \u001b[43mpipeline\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdocuments\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[43mDocument\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexample\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     26\u001b[0m \u001b[38;5;66;03m# Create your index\u001b[39;00m\n\u001b[1;32m     27\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mllama_index\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcore\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m VectorStoreIndex\n",
      "File \u001b[0;32m~/atest/llamaindex-agents/.venv/lib/python3.11/site-packages/llama_index/core/instrumentation/dispatcher.py:260\u001b[0m, in \u001b[0;36mDispatcher.span.<locals>.wrapper\u001b[0;34m(func, instance, args, kwargs)\u001b[0m\n\u001b[1;32m    252\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mspan_enter(\n\u001b[1;32m    253\u001b[0m     id_\u001b[38;5;241m=\u001b[39mid_,\n\u001b[1;32m    254\u001b[0m     bound_args\u001b[38;5;241m=\u001b[39mbound_args,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    257\u001b[0m     tags\u001b[38;5;241m=\u001b[39mtags,\n\u001b[1;32m    258\u001b[0m )\n\u001b[1;32m    259\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 260\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    261\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    262\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mevent(SpanDropEvent(span_id\u001b[38;5;241m=\u001b[39mid_, err_str\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mstr\u001b[39m(e)))\n",
      "File \u001b[0;32m~/atest/llamaindex-agents/.venv/lib/python3.11/site-packages/llama_index/core/ingestion/pipeline.py:542\u001b[0m, in \u001b[0;36mIngestionPipeline.run\u001b[0;34m(self, show_progress, documents, nodes, cache_collection, in_place, store_doc_text, num_workers, **kwargs)\u001b[0m\n\u001b[1;32m    540\u001b[0m         nodes \u001b[38;5;241m=\u001b[39m reduce(\u001b[38;5;28;01mlambda\u001b[39;00m x, y: x \u001b[38;5;241m+\u001b[39m y, nodes_parallel, [])\n\u001b[1;32m    541\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 542\u001b[0m     nodes \u001b[38;5;241m=\u001b[39m \u001b[43mrun_transformations\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    543\u001b[0m \u001b[43m        \u001b[49m\u001b[43mnodes_to_run\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    544\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtransformations\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    545\u001b[0m \u001b[43m        \u001b[49m\u001b[43mshow_progress\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mshow_progress\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    546\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcache\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcache\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mnot\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdisable_cache\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    547\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcache_collection\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcache_collection\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    548\u001b[0m \u001b[43m        \u001b[49m\u001b[43min_place\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43min_place\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    549\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    550\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    552\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mvector_store \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    553\u001b[0m     nodes_with_embeddings \u001b[38;5;241m=\u001b[39m [n \u001b[38;5;28;01mfor\u001b[39;00m n \u001b[38;5;129;01min\u001b[39;00m nodes \u001b[38;5;28;01mif\u001b[39;00m n\u001b[38;5;241m.\u001b[39membedding \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m]\n",
      "File \u001b[0;32m~/atest/llamaindex-agents/.venv/lib/python3.11/site-packages/llama_index/core/ingestion/pipeline.py:97\u001b[0m, in \u001b[0;36mrun_transformations\u001b[0;34m(nodes, transformations, in_place, cache, cache_collection, **kwargs)\u001b[0m\n\u001b[1;32m     95\u001b[0m         nodes \u001b[38;5;241m=\u001b[39m cached_nodes\n\u001b[1;32m     96\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m---> 97\u001b[0m         nodes \u001b[38;5;241m=\u001b[39m \u001b[43mtransform\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnodes\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     98\u001b[0m         cache\u001b[38;5;241m.\u001b[39mput(\u001b[38;5;28mhash\u001b[39m, nodes, collection\u001b[38;5;241m=\u001b[39mcache_collection)\n\u001b[1;32m     99\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "File \u001b[0;32m~/atest/llamaindex-agents/.venv/lib/python3.11/site-packages/llama_index/core/instrumentation/dispatcher.py:260\u001b[0m, in \u001b[0;36mDispatcher.span.<locals>.wrapper\u001b[0;34m(func, instance, args, kwargs)\u001b[0m\n\u001b[1;32m    252\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mspan_enter(\n\u001b[1;32m    253\u001b[0m     id_\u001b[38;5;241m=\u001b[39mid_,\n\u001b[1;32m    254\u001b[0m     bound_args\u001b[38;5;241m=\u001b[39mbound_args,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    257\u001b[0m     tags\u001b[38;5;241m=\u001b[39mtags,\n\u001b[1;32m    258\u001b[0m )\n\u001b[1;32m    259\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 260\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    261\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    262\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mevent(SpanDropEvent(span_id\u001b[38;5;241m=\u001b[39mid_, err_str\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mstr\u001b[39m(e)))\n",
      "File \u001b[0;32m~/atest/llamaindex-agents/.venv/lib/python3.11/site-packages/llama_index/core/extractors/interface.py:159\u001b[0m, in \u001b[0;36mBaseExtractor.__call__\u001b[0;34m(self, nodes, **kwargs)\u001b[0m\n\u001b[1;32m    151\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, nodes: List[BaseNode], \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m List[BaseNode]:\n\u001b[1;32m    152\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Post process nodes parsed from documents.\u001b[39;00m\n\u001b[1;32m    153\u001b[0m \n\u001b[1;32m    154\u001b[0m \u001b[38;5;124;03m    Allows extractors to be chained.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    157\u001b[0m \u001b[38;5;124;03m        nodes (List[BaseNode]): nodes to post-process\u001b[39;00m\n\u001b[1;32m    158\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 159\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mprocess_nodes\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnodes\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/atest/llamaindex-agents/.venv/lib/python3.11/site-packages/llama_index/core/extractors/interface.py:142\u001b[0m, in \u001b[0;36mBaseExtractor.process_nodes\u001b[0;34m(self, nodes, excluded_embed_metadata_keys, excluded_llm_metadata_keys, **kwargs)\u001b[0m\n\u001b[1;32m    135\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mprocess_nodes\u001b[39m(\n\u001b[1;32m    136\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    137\u001b[0m     nodes: List[BaseNode],\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    140\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any,\n\u001b[1;32m    141\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m List[BaseNode]:\n\u001b[0;32m--> 142\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43masyncio_run\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    143\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43maprocess_nodes\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    144\u001b[0m \u001b[43m            \u001b[49m\u001b[43mnodes\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    145\u001b[0m \u001b[43m            \u001b[49m\u001b[43mexcluded_embed_metadata_keys\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mexcluded_embed_metadata_keys\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    146\u001b[0m \u001b[43m            \u001b[49m\u001b[43mexcluded_llm_metadata_keys\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mexcluded_llm_metadata_keys\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    147\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    148\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    149\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/atest/llamaindex-agents/.venv/lib/python3.11/site-packages/llama_index/core/async_utils.py:33\u001b[0m, in \u001b[0;36masyncio_run\u001b[0;34m(coro)\u001b[0m\n\u001b[1;32m     30\u001b[0m     loop \u001b[38;5;241m=\u001b[39m asyncio\u001b[38;5;241m.\u001b[39mget_event_loop()\n\u001b[1;32m     32\u001b[0m     \u001b[38;5;66;03m# If we're here, there's an existing loop but it's not running\u001b[39;00m\n\u001b[0;32m---> 33\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mloop\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_until_complete\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcoro\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     35\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m     36\u001b[0m     \u001b[38;5;66;03m# If we can't get the event loop, we're likely in a different thread, or its already running\u001b[39;00m\n\u001b[1;32m     37\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "File \u001b[0;32m~/atest/llamaindex-agents/.venv/lib/python3.11/site-packages/nest_asyncio.py:98\u001b[0m, in \u001b[0;36m_patch_loop.<locals>.run_until_complete\u001b[0;34m(self, future)\u001b[0m\n\u001b[1;32m     95\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m f\u001b[38;5;241m.\u001b[39mdone():\n\u001b[1;32m     96\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[1;32m     97\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mEvent loop stopped before Future completed.\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m---> 98\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mresult\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/deep/lib/python3.11/asyncio/futures.py:203\u001b[0m, in \u001b[0;36mFuture.result\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    201\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m__log_traceback \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[1;32m    202\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_exception \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 203\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_exception\u001b[38;5;241m.\u001b[39mwith_traceback(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_exception_tb)\n\u001b[1;32m    204\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_result\n",
      "File \u001b[0;32m~/miniconda3/envs/deep/lib/python3.11/asyncio/tasks.py:277\u001b[0m, in \u001b[0;36mTask.__step\u001b[0;34m(***failed resolving arguments***)\u001b[0m\n\u001b[1;32m    273\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    274\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m exc \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    275\u001b[0m         \u001b[38;5;66;03m# We use the `send` method directly, because coroutines\u001b[39;00m\n\u001b[1;32m    276\u001b[0m         \u001b[38;5;66;03m# don't have `__iter__` and `__next__` methods.\u001b[39;00m\n\u001b[0;32m--> 277\u001b[0m         result \u001b[38;5;241m=\u001b[39m coro\u001b[38;5;241m.\u001b[39msend(\u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[1;32m    278\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    279\u001b[0m         result \u001b[38;5;241m=\u001b[39m coro\u001b[38;5;241m.\u001b[39mthrow(exc)\n",
      "File \u001b[0;32m~/atest/llamaindex-agents/.venv/lib/python3.11/site-packages/llama_index/core/extractors/interface.py:120\u001b[0m, in \u001b[0;36mBaseExtractor.aprocess_nodes\u001b[0;34m(self, nodes, excluded_embed_metadata_keys, excluded_llm_metadata_keys, **kwargs)\u001b[0m\n\u001b[1;32m    117\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    118\u001b[0m     new_nodes \u001b[38;5;241m=\u001b[39m [deepcopy(node) \u001b[38;5;28;01mfor\u001b[39;00m node \u001b[38;5;129;01min\u001b[39;00m nodes]\n\u001b[0;32m--> 120\u001b[0m cur_metadata_list \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maextract(new_nodes)\n\u001b[1;32m    121\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m idx, node \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(new_nodes):\n\u001b[1;32m    122\u001b[0m     node\u001b[38;5;241m.\u001b[39mmetadata\u001b[38;5;241m.\u001b[39mupdate(cur_metadata_list[idx])\n",
      "File \u001b[0;32m~/atest/llamaindex-agents/.venv/lib/python3.11/site-packages/llama_index/core/instrumentation/dispatcher.py:290\u001b[0m, in \u001b[0;36mDispatcher.span.<locals>.async_wrapper\u001b[0;34m(func, instance, args, kwargs)\u001b[0m\n\u001b[1;32m    282\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mspan_enter(\n\u001b[1;32m    283\u001b[0m     id_\u001b[38;5;241m=\u001b[39mid_,\n\u001b[1;32m    284\u001b[0m     bound_args\u001b[38;5;241m=\u001b[39mbound_args,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    287\u001b[0m     tags\u001b[38;5;241m=\u001b[39mtags,\n\u001b[1;32m    288\u001b[0m )\n\u001b[1;32m    289\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 290\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mawait\u001b[39;00m func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    291\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    292\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mevent(SpanDropEvent(span_id\u001b[38;5;241m=\u001b[39mid_, err_str\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mstr\u001b[39m(e)))\n",
      "File \u001b[0;32m~/atest/llamaindex-agents/.venv/lib/python3.11/site-packages/llama_index/core/extractors/metadata_extractors.py:104\u001b[0m, in \u001b[0;36mTitleExtractor.aextract\u001b[0;34m(self, nodes)\u001b[0m\n\u001b[1;32m    102\u001b[0m \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21maextract\u001b[39m(\u001b[38;5;28mself\u001b[39m, nodes: Sequence[BaseNode]) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m List[Dict]:\n\u001b[1;32m    103\u001b[0m     nodes_by_doc_id \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mseparate_nodes_by_ref_id(nodes)\n\u001b[0;32m--> 104\u001b[0m     titles_by_doc_id \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mextract_titles(nodes_by_doc_id)\n\u001b[1;32m    105\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m [{\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdocument_title\u001b[39m\u001b[38;5;124m\"\u001b[39m: titles_by_doc_id[node\u001b[38;5;241m.\u001b[39mref_doc_id]} \u001b[38;5;28;01mfor\u001b[39;00m node \u001b[38;5;129;01min\u001b[39;00m nodes]\n",
      "File \u001b[0;32m~/atest/llamaindex-agents/.venv/lib/python3.11/site-packages/llama_index/core/extractors/metadata_extractors.py:131\u001b[0m, in \u001b[0;36mTitleExtractor.extract_titles\u001b[0;34m(self, nodes_by_doc_id)\u001b[0m\n\u001b[1;32m    129\u001b[0m titles_by_doc_id \u001b[38;5;241m=\u001b[39m {}\n\u001b[1;32m    130\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m key, nodes \u001b[38;5;129;01min\u001b[39;00m nodes_by_doc_id\u001b[38;5;241m.\u001b[39mitems():\n\u001b[0;32m--> 131\u001b[0m     title_candidates \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mget_title_candidates(nodes)\n\u001b[1;32m    132\u001b[0m     combined_titles \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m, \u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(title_candidates)\n\u001b[1;32m    133\u001b[0m     titles_by_doc_id[key] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mllm\u001b[38;5;241m.\u001b[39mapredict(\n\u001b[1;32m    134\u001b[0m         PromptTemplate(template\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcombine_template),\n\u001b[1;32m    135\u001b[0m         context_str\u001b[38;5;241m=\u001b[39mcombined_titles,\n\u001b[1;32m    136\u001b[0m     )\n",
      "File \u001b[0;32m~/atest/llamaindex-agents/.venv/lib/python3.11/site-packages/llama_index/core/extractors/metadata_extractors.py:147\u001b[0m, in \u001b[0;36mTitleExtractor.get_title_candidates\u001b[0;34m(self, nodes)\u001b[0m\n\u001b[1;32m    139\u001b[0m \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mget_title_candidates\u001b[39m(\u001b[38;5;28mself\u001b[39m, nodes: List[BaseNode]) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m List[\u001b[38;5;28mstr\u001b[39m]:\n\u001b[1;32m    140\u001b[0m     title_jobs \u001b[38;5;241m=\u001b[39m [\n\u001b[1;32m    141\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mllm\u001b[38;5;241m.\u001b[39mapredict(\n\u001b[1;32m    142\u001b[0m             PromptTemplate(template\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnode_template),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    145\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m node \u001b[38;5;129;01min\u001b[39;00m nodes\n\u001b[1;32m    146\u001b[0m     ]\n\u001b[0;32m--> 147\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mawait\u001b[39;00m run_jobs(\n\u001b[1;32m    148\u001b[0m         title_jobs, show_progress\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mshow_progress, workers\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_workers\n\u001b[1;32m    149\u001b[0m     )\n",
      "File \u001b[0;32m~/atest/llamaindex-agents/.venv/lib/python3.11/site-packages/llama_index/core/instrumentation/dispatcher.py:290\u001b[0m, in \u001b[0;36mDispatcher.span.<locals>.async_wrapper\u001b[0;34m(func, instance, args, kwargs)\u001b[0m\n\u001b[1;32m    282\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mspan_enter(\n\u001b[1;32m    283\u001b[0m     id_\u001b[38;5;241m=\u001b[39mid_,\n\u001b[1;32m    284\u001b[0m     bound_args\u001b[38;5;241m=\u001b[39mbound_args,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    287\u001b[0m     tags\u001b[38;5;241m=\u001b[39mtags,\n\u001b[1;32m    288\u001b[0m )\n\u001b[1;32m    289\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 290\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mawait\u001b[39;00m func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    291\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    292\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mevent(SpanDropEvent(span_id\u001b[38;5;241m=\u001b[39mid_, err_str\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mstr\u001b[39m(e)))\n",
      "File \u001b[0;32m~/atest/llamaindex-agents/.venv/lib/python3.11/site-packages/llama_index/core/async_utils.py:146\u001b[0m, in \u001b[0;36mrun_jobs\u001b[0;34m(jobs, show_progress, workers, desc)\u001b[0m\n\u001b[1;32m    143\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m show_progress:\n\u001b[1;32m    144\u001b[0m     \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtqdm\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01masyncio\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m tqdm_asyncio\n\u001b[0;32m--> 146\u001b[0m     results \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mawait\u001b[39;00m tqdm_asyncio\u001b[38;5;241m.\u001b[39mgather(\u001b[38;5;241m*\u001b[39mpool_jobs, desc\u001b[38;5;241m=\u001b[39mdesc)\n\u001b[1;32m    147\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    148\u001b[0m     results \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mawait\u001b[39;00m asyncio\u001b[38;5;241m.\u001b[39mgather(\u001b[38;5;241m*\u001b[39mpool_jobs)\n",
      "File \u001b[0;32m~/atest/llamaindex-agents/.venv/lib/python3.11/site-packages/tqdm/asyncio.py:79\u001b[0m, in \u001b[0;36mtqdm_asyncio.gather\u001b[0;34m(cls, loop, timeout, total, *fs, **tqdm_kwargs)\u001b[0m\n\u001b[1;32m     76\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m i, \u001b[38;5;28;01mawait\u001b[39;00m f\n\u001b[1;32m     78\u001b[0m ifs \u001b[38;5;241m=\u001b[39m [wrap_awaitable(i, f) \u001b[38;5;28;01mfor\u001b[39;00m i, f \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(fs)]\n\u001b[0;32m---> 79\u001b[0m res \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28;01mawait\u001b[39;00m f \u001b[38;5;28;01mfor\u001b[39;00m f \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39mas_completed(ifs, loop\u001b[38;5;241m=\u001b[39mloop, timeout\u001b[38;5;241m=\u001b[39mtimeout,\n\u001b[1;32m     80\u001b[0m                                          total\u001b[38;5;241m=\u001b[39mtotal, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mtqdm_kwargs)]\n\u001b[1;32m     81\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m [i \u001b[38;5;28;01mfor\u001b[39;00m _, i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28msorted\u001b[39m(res)]\n",
      "File \u001b[0;32m~/atest/llamaindex-agents/.venv/lib/python3.11/site-packages/tqdm/asyncio.py:79\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     76\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m i, \u001b[38;5;28;01mawait\u001b[39;00m f\n\u001b[1;32m     78\u001b[0m ifs \u001b[38;5;241m=\u001b[39m [wrap_awaitable(i, f) \u001b[38;5;28;01mfor\u001b[39;00m i, f \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(fs)]\n\u001b[0;32m---> 79\u001b[0m res \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28;01mawait\u001b[39;00m f \u001b[38;5;28;01mfor\u001b[39;00m f \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39mas_completed(ifs, loop\u001b[38;5;241m=\u001b[39mloop, timeout\u001b[38;5;241m=\u001b[39mtimeout,\n\u001b[1;32m     80\u001b[0m                                          total\u001b[38;5;241m=\u001b[39mtotal, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mtqdm_kwargs)]\n\u001b[1;32m     81\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m [i \u001b[38;5;28;01mfor\u001b[39;00m _, i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28msorted\u001b[39m(res)]\n",
      "File \u001b[0;32m~/miniconda3/envs/deep/lib/python3.11/asyncio/tasks.py:615\u001b[0m, in \u001b[0;36mas_completed.<locals>._wait_for_one\u001b[0;34m()\u001b[0m\n\u001b[1;32m    612\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m f \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    613\u001b[0m     \u001b[38;5;66;03m# Dummy value from _on_timeout().\u001b[39;00m\n\u001b[1;32m    614\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m exceptions\u001b[38;5;241m.\u001b[39mTimeoutError\n\u001b[0;32m--> 615\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mresult\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/deep/lib/python3.11/asyncio/futures.py:203\u001b[0m, in \u001b[0;36mFuture.result\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    201\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m__log_traceback \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[1;32m    202\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_exception \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 203\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_exception\u001b[38;5;241m.\u001b[39mwith_traceback(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_exception_tb)\n\u001b[1;32m    204\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_result\n",
      "File \u001b[0;32m~/miniconda3/envs/deep/lib/python3.11/asyncio/tasks.py:277\u001b[0m, in \u001b[0;36mTask.__step\u001b[0;34m(***failed resolving arguments***)\u001b[0m\n\u001b[1;32m    273\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    274\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m exc \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    275\u001b[0m         \u001b[38;5;66;03m# We use the `send` method directly, because coroutines\u001b[39;00m\n\u001b[1;32m    276\u001b[0m         \u001b[38;5;66;03m# don't have `__iter__` and `__next__` methods.\u001b[39;00m\n\u001b[0;32m--> 277\u001b[0m         result \u001b[38;5;241m=\u001b[39m coro\u001b[38;5;241m.\u001b[39msend(\u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[1;32m    278\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    279\u001b[0m         result \u001b[38;5;241m=\u001b[39m coro\u001b[38;5;241m.\u001b[39mthrow(exc)\n",
      "File \u001b[0;32m~/atest/llamaindex-agents/.venv/lib/python3.11/site-packages/tqdm/asyncio.py:76\u001b[0m, in \u001b[0;36mtqdm_asyncio.gather.<locals>.wrap_awaitable\u001b[0;34m(i, f)\u001b[0m\n\u001b[1;32m     75\u001b[0m \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mwrap_awaitable\u001b[39m(i, f):\n\u001b[0;32m---> 76\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m i, \u001b[38;5;28;01mawait\u001b[39;00m f\n",
      "File \u001b[0;32m~/atest/llamaindex-agents/.venv/lib/python3.11/site-packages/llama_index/core/instrumentation/dispatcher.py:290\u001b[0m, in \u001b[0;36mDispatcher.span.<locals>.async_wrapper\u001b[0;34m(func, instance, args, kwargs)\u001b[0m\n\u001b[1;32m    282\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mspan_enter(\n\u001b[1;32m    283\u001b[0m     id_\u001b[38;5;241m=\u001b[39mid_,\n\u001b[1;32m    284\u001b[0m     bound_args\u001b[38;5;241m=\u001b[39mbound_args,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    287\u001b[0m     tags\u001b[38;5;241m=\u001b[39mtags,\n\u001b[1;32m    288\u001b[0m )\n\u001b[1;32m    289\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 290\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mawait\u001b[39;00m func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    291\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    292\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mevent(SpanDropEvent(span_id\u001b[38;5;241m=\u001b[39mid_, err_str\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mstr\u001b[39m(e)))\n",
      "File \u001b[0;32m~/atest/llamaindex-agents/.venv/lib/python3.11/site-packages/llama_index/core/async_utils.py:139\u001b[0m, in \u001b[0;36mrun_jobs.<locals>.worker\u001b[0;34m(job)\u001b[0m\n\u001b[1;32m    136\u001b[0m \u001b[38;5;129m@dispatcher\u001b[39m\u001b[38;5;241m.\u001b[39mspan\n\u001b[1;32m    137\u001b[0m \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mworker\u001b[39m(job: Coroutine) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Any:\n\u001b[1;32m    138\u001b[0m     \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mwith\u001b[39;00m semaphore:\n\u001b[0;32m--> 139\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mawait\u001b[39;00m job\n",
      "File \u001b[0;32m~/atest/llamaindex-agents/.venv/lib/python3.11/site-packages/llama_index/core/instrumentation/dispatcher.py:290\u001b[0m, in \u001b[0;36mDispatcher.span.<locals>.async_wrapper\u001b[0;34m(func, instance, args, kwargs)\u001b[0m\n\u001b[1;32m    282\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mspan_enter(\n\u001b[1;32m    283\u001b[0m     id_\u001b[38;5;241m=\u001b[39mid_,\n\u001b[1;32m    284\u001b[0m     bound_args\u001b[38;5;241m=\u001b[39mbound_args,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    287\u001b[0m     tags\u001b[38;5;241m=\u001b[39mtags,\n\u001b[1;32m    288\u001b[0m )\n\u001b[1;32m    289\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 290\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mawait\u001b[39;00m func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    291\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    292\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mevent(SpanDropEvent(span_id\u001b[38;5;241m=\u001b[39mid_, err_str\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mstr\u001b[39m(e)))\n",
      "File \u001b[0;32m~/atest/llamaindex-agents/.venv/lib/python3.11/site-packages/llama_index/core/llms/llm.py:648\u001b[0m, in \u001b[0;36mLLM.apredict\u001b[0;34m(self, prompt, **prompt_args)\u001b[0m\n\u001b[1;32m    646\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmetadata\u001b[38;5;241m.\u001b[39mis_chat_model:\n\u001b[1;32m    647\u001b[0m     messages \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_messages(prompt, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mprompt_args)\n\u001b[0;32m--> 648\u001b[0m     chat_response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39machat(messages)\n\u001b[1;32m    649\u001b[0m     output \u001b[38;5;241m=\u001b[39m chat_response\u001b[38;5;241m.\u001b[39mmessage\u001b[38;5;241m.\u001b[39mcontent \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    650\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "File \u001b[0;32m~/atest/llamaindex-agents/.venv/lib/python3.11/site-packages/llama_index/core/instrumentation/dispatcher.py:290\u001b[0m, in \u001b[0;36mDispatcher.span.<locals>.async_wrapper\u001b[0;34m(func, instance, args, kwargs)\u001b[0m\n\u001b[1;32m    282\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mspan_enter(\n\u001b[1;32m    283\u001b[0m     id_\u001b[38;5;241m=\u001b[39mid_,\n\u001b[1;32m    284\u001b[0m     bound_args\u001b[38;5;241m=\u001b[39mbound_args,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    287\u001b[0m     tags\u001b[38;5;241m=\u001b[39mtags,\n\u001b[1;32m    288\u001b[0m )\n\u001b[1;32m    289\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 290\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mawait\u001b[39;00m func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    291\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    292\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mevent(SpanDropEvent(span_id\u001b[38;5;241m=\u001b[39mid_, err_str\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mstr\u001b[39m(e)))\n",
      "File \u001b[0;32m~/atest/llamaindex-agents/.venv/lib/python3.11/site-packages/llama_index/core/llms/callbacks.py:76\u001b[0m, in \u001b[0;36mllm_chat_callback.<locals>.wrap.<locals>.wrapped_async_llm_chat\u001b[0;34m(_self, messages, **kwargs)\u001b[0m\n\u001b[1;32m     67\u001b[0m event_id \u001b[38;5;241m=\u001b[39m callback_manager\u001b[38;5;241m.\u001b[39mon_event_start(\n\u001b[1;32m     68\u001b[0m     CBEventType\u001b[38;5;241m.\u001b[39mLLM,\n\u001b[1;32m     69\u001b[0m     payload\u001b[38;5;241m=\u001b[39m{\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     73\u001b[0m     },\n\u001b[1;32m     74\u001b[0m )\n\u001b[1;32m     75\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m---> 76\u001b[0m     f_return_val \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mawait\u001b[39;00m f(_self, messages, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m     77\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m     78\u001b[0m     callback_manager\u001b[38;5;241m.\u001b[39mon_event_end(\n\u001b[1;32m     79\u001b[0m         CBEventType\u001b[38;5;241m.\u001b[39mLLM,\n\u001b[1;32m     80\u001b[0m         payload\u001b[38;5;241m=\u001b[39m{EventPayload\u001b[38;5;241m.\u001b[39mEXCEPTION: e},\n\u001b[1;32m     81\u001b[0m         event_id\u001b[38;5;241m=\u001b[39mevent_id,\n\u001b[1;32m     82\u001b[0m     )\n",
      "File \u001b[0;32m~/atest/llamaindex-agents/.venv/lib/python3.11/site-packages/llama_index/core/llms/custom.py:49\u001b[0m, in \u001b[0;36mCustomLLM.achat\u001b[0;34m(self, messages, **kwargs)\u001b[0m\n\u001b[1;32m     43\u001b[0m \u001b[38;5;129m@llm_chat_callback\u001b[39m()\n\u001b[1;32m     44\u001b[0m \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21machat\u001b[39m(\n\u001b[1;32m     45\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m     46\u001b[0m     messages: Sequence[ChatMessage],\n\u001b[1;32m     47\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any,\n\u001b[1;32m     48\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m ChatResponse:\n\u001b[0;32m---> 49\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mchat\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmessages\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/atest/llamaindex-agents/.venv/lib/python3.11/site-packages/llama_index/core/instrumentation/dispatcher.py:260\u001b[0m, in \u001b[0;36mDispatcher.span.<locals>.wrapper\u001b[0;34m(func, instance, args, kwargs)\u001b[0m\n\u001b[1;32m    252\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mspan_enter(\n\u001b[1;32m    253\u001b[0m     id_\u001b[38;5;241m=\u001b[39mid_,\n\u001b[1;32m    254\u001b[0m     bound_args\u001b[38;5;241m=\u001b[39mbound_args,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    257\u001b[0m     tags\u001b[38;5;241m=\u001b[39mtags,\n\u001b[1;32m    258\u001b[0m )\n\u001b[1;32m    259\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 260\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    261\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    262\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mevent(SpanDropEvent(span_id\u001b[38;5;241m=\u001b[39mid_, err_str\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mstr\u001b[39m(e)))\n",
      "File \u001b[0;32m~/atest/llamaindex-agents/.venv/lib/python3.11/site-packages/llama_index/core/llms/callbacks.py:172\u001b[0m, in \u001b[0;36mllm_chat_callback.<locals>.wrap.<locals>.wrapped_llm_chat\u001b[0;34m(_self, messages, **kwargs)\u001b[0m\n\u001b[1;32m    163\u001b[0m event_id \u001b[38;5;241m=\u001b[39m callback_manager\u001b[38;5;241m.\u001b[39mon_event_start(\n\u001b[1;32m    164\u001b[0m     CBEventType\u001b[38;5;241m.\u001b[39mLLM,\n\u001b[1;32m    165\u001b[0m     payload\u001b[38;5;241m=\u001b[39m{\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    169\u001b[0m     },\n\u001b[1;32m    170\u001b[0m )\n\u001b[1;32m    171\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 172\u001b[0m     f_return_val \u001b[38;5;241m=\u001b[39m \u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[43m_self\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmessages\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    173\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    174\u001b[0m     callback_manager\u001b[38;5;241m.\u001b[39mon_event_end(\n\u001b[1;32m    175\u001b[0m         CBEventType\u001b[38;5;241m.\u001b[39mLLM,\n\u001b[1;32m    176\u001b[0m         payload\u001b[38;5;241m=\u001b[39m{EventPayload\u001b[38;5;241m.\u001b[39mEXCEPTION: e},\n\u001b[1;32m    177\u001b[0m         event_id\u001b[38;5;241m=\u001b[39mevent_id,\n\u001b[1;32m    178\u001b[0m     )\n",
      "File \u001b[0;32m~/atest/llamaindex-agents/.venv/lib/python3.11/site-packages/llama_index/llms/gemini/base.py:201\u001b[0m, in \u001b[0;36mGemini.chat\u001b[0;34m(self, messages, **kwargs)\u001b[0m\n\u001b[1;32m    199\u001b[0m \u001b[38;5;241m*\u001b[39mhistory, next_msg \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mmap\u001b[39m(chat_message_to_gemini, merged_messages)\n\u001b[1;32m    200\u001b[0m chat \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_model\u001b[38;5;241m.\u001b[39mstart_chat(history\u001b[38;5;241m=\u001b[39mhistory)\n\u001b[0;32m--> 201\u001b[0m response \u001b[38;5;241m=\u001b[39m \u001b[43mchat\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msend_message\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnext_msg\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    202\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m chat_from_gemini_response(response)\n",
      "File \u001b[0;32m~/atest/llamaindex-agents/.venv/lib/python3.11/site-packages/google/generativeai/generative_models.py:496\u001b[0m, in \u001b[0;36mChatSession.send_message\u001b[0;34m(self, content, generation_config, safety_settings, stream, tools, tool_config)\u001b[0m\n\u001b[1;32m    493\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m generation_config\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcandidate_count\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;241m1\u001b[39m) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[1;32m    494\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCan\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt chat with `candidate_count > 1`\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m--> 496\u001b[0m response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgenerate_content\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    497\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcontents\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhistory\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    498\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgeneration_config\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgeneration_config\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    499\u001b[0m \u001b[43m    \u001b[49m\u001b[43msafety_settings\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msafety_settings\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    500\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstream\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    501\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtools\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtools_lib\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    502\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtool_config\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtool_config\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    503\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    505\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_response(response\u001b[38;5;241m=\u001b[39mresponse, stream\u001b[38;5;241m=\u001b[39mstream)\n\u001b[1;32m    507\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39menable_automatic_function_calling \u001b[38;5;129;01mand\u001b[39;00m tools_lib \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[0;32m~/atest/llamaindex-agents/.venv/lib/python3.11/site-packages/google/generativeai/generative_models.py:262\u001b[0m, in \u001b[0;36mGenerativeModel.generate_content\u001b[0;34m(self, contents, generation_config, safety_settings, stream, tools, tool_config, request_options)\u001b[0m\n\u001b[1;32m    260\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m generation_types\u001b[38;5;241m.\u001b[39mGenerateContentResponse\u001b[38;5;241m.\u001b[39mfrom_iterator(iterator)\n\u001b[1;32m    261\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 262\u001b[0m         response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_client\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgenerate_content\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    263\u001b[0m \u001b[43m            \u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    264\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mrequest_options\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    265\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    266\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m generation_types\u001b[38;5;241m.\u001b[39mGenerateContentResponse\u001b[38;5;241m.\u001b[39mfrom_response(response)\n\u001b[1;32m    267\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m google\u001b[38;5;241m.\u001b[39mapi_core\u001b[38;5;241m.\u001b[39mexceptions\u001b[38;5;241m.\u001b[39mInvalidArgument \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "File \u001b[0;32m~/atest/llamaindex-agents/.venv/lib/python3.11/site-packages/google/ai/generativelanguage_v1beta/services/generative_service/client.py:812\u001b[0m, in \u001b[0;36mGenerativeServiceClient.generate_content\u001b[0;34m(self, request, model, contents, retry, timeout, metadata)\u001b[0m\n\u001b[1;32m    809\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_validate_universe_domain()\n\u001b[1;32m    811\u001b[0m \u001b[38;5;66;03m# Send the request.\u001b[39;00m\n\u001b[0;32m--> 812\u001b[0m response \u001b[38;5;241m=\u001b[39m \u001b[43mrpc\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    813\u001b[0m \u001b[43m    \u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    814\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretry\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mretry\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    815\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    816\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmetadata\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmetadata\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    817\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    819\u001b[0m \u001b[38;5;66;03m# Done; return the response.\u001b[39;00m\n\u001b[1;32m    820\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m response\n",
      "File \u001b[0;32m~/atest/llamaindex-agents/.venv/lib/python3.11/site-packages/google/api_core/gapic_v1/method.py:131\u001b[0m, in \u001b[0;36m_GapicCallable.__call__\u001b[0;34m(self, timeout, retry, compression, *args, **kwargs)\u001b[0m\n\u001b[1;32m    128\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compression \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    129\u001b[0m     kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcompression\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m compression\n\u001b[0;32m--> 131\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mwrapped_func\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/atest/llamaindex-agents/.venv/lib/python3.11/site-packages/google/api_core/retry/retry_unary.py:293\u001b[0m, in \u001b[0;36mRetry.__call__.<locals>.retry_wrapped_func\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    289\u001b[0m target \u001b[38;5;241m=\u001b[39m functools\u001b[38;5;241m.\u001b[39mpartial(func, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    290\u001b[0m sleep_generator \u001b[38;5;241m=\u001b[39m exponential_sleep_generator(\n\u001b[1;32m    291\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_initial, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_maximum, multiplier\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_multiplier\n\u001b[1;32m    292\u001b[0m )\n\u001b[0;32m--> 293\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mretry_target\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    294\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtarget\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    295\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_predicate\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    296\u001b[0m \u001b[43m    \u001b[49m\u001b[43msleep_generator\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    297\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_timeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    298\u001b[0m \u001b[43m    \u001b[49m\u001b[43mon_error\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mon_error\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    299\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/atest/llamaindex-agents/.venv/lib/python3.11/site-packages/google/api_core/retry/retry_unary.py:153\u001b[0m, in \u001b[0;36mretry_target\u001b[0;34m(target, predicate, sleep_generator, timeout, on_error, exception_factory, **kwargs)\u001b[0m\n\u001b[1;32m    149\u001b[0m \u001b[38;5;66;03m# pylint: disable=broad-except\u001b[39;00m\n\u001b[1;32m    150\u001b[0m \u001b[38;5;66;03m# This function explicitly must deal with broad exceptions.\u001b[39;00m\n\u001b[1;32m    151\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m exc:\n\u001b[1;32m    152\u001b[0m     \u001b[38;5;66;03m# defer to shared logic for handling errors\u001b[39;00m\n\u001b[0;32m--> 153\u001b[0m     \u001b[43m_retry_error_helper\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    154\u001b[0m \u001b[43m        \u001b[49m\u001b[43mexc\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    155\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdeadline\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    156\u001b[0m \u001b[43m        \u001b[49m\u001b[43msleep\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    157\u001b[0m \u001b[43m        \u001b[49m\u001b[43merror_list\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    158\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpredicate\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    159\u001b[0m \u001b[43m        \u001b[49m\u001b[43mon_error\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    160\u001b[0m \u001b[43m        \u001b[49m\u001b[43mexception_factory\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    161\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    162\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    163\u001b[0m     \u001b[38;5;66;03m# if exception not raised, sleep before next attempt\u001b[39;00m\n\u001b[1;32m    164\u001b[0m     time\u001b[38;5;241m.\u001b[39msleep(sleep)\n",
      "File \u001b[0;32m~/atest/llamaindex-agents/.venv/lib/python3.11/site-packages/google/api_core/retry/retry_base.py:212\u001b[0m, in \u001b[0;36m_retry_error_helper\u001b[0;34m(exc, deadline, next_sleep, error_list, predicate_fn, on_error_fn, exc_factory_fn, original_timeout)\u001b[0m\n\u001b[1;32m    206\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m predicate_fn(exc):\n\u001b[1;32m    207\u001b[0m     final_exc, source_exc \u001b[38;5;241m=\u001b[39m exc_factory_fn(\n\u001b[1;32m    208\u001b[0m         error_list,\n\u001b[1;32m    209\u001b[0m         RetryFailureReason\u001b[38;5;241m.\u001b[39mNON_RETRYABLE_ERROR,\n\u001b[1;32m    210\u001b[0m         original_timeout,\n\u001b[1;32m    211\u001b[0m     )\n\u001b[0;32m--> 212\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m final_exc \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msource_exc\u001b[39;00m\n\u001b[1;32m    213\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m on_error_fn \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    214\u001b[0m     on_error_fn(exc)\n",
      "File \u001b[0;32m~/atest/llamaindex-agents/.venv/lib/python3.11/site-packages/google/api_core/retry/retry_unary.py:144\u001b[0m, in \u001b[0;36mretry_target\u001b[0;34m(target, predicate, sleep_generator, timeout, on_error, exception_factory, **kwargs)\u001b[0m\n\u001b[1;32m    142\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m sleep \u001b[38;5;129;01min\u001b[39;00m sleep_generator:\n\u001b[1;32m    143\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 144\u001b[0m         result \u001b[38;5;241m=\u001b[39m \u001b[43mtarget\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    145\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m inspect\u001b[38;5;241m.\u001b[39misawaitable(result):\n\u001b[1;32m    146\u001b[0m             warnings\u001b[38;5;241m.\u001b[39mwarn(_ASYNC_RETRY_WARNING)\n",
      "File \u001b[0;32m~/atest/llamaindex-agents/.venv/lib/python3.11/site-packages/google/api_core/timeout.py:120\u001b[0m, in \u001b[0;36mTimeToDeadlineTimeout.__call__.<locals>.func_with_timeout\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    117\u001b[0m     \u001b[38;5;66;03m# Avoid setting negative timeout\u001b[39;00m\n\u001b[1;32m    118\u001b[0m     kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtimeout\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mmax\u001b[39m(\u001b[38;5;241m0\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_timeout \u001b[38;5;241m-\u001b[39m time_since_first_attempt)\n\u001b[0;32m--> 120\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/atest/llamaindex-agents/.venv/lib/python3.11/site-packages/google/api_core/grpc_helpers.py:78\u001b[0m, in \u001b[0;36m_wrap_unary_errors.<locals>.error_remapped_callable\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     76\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m callable_(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m     77\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m grpc\u001b[38;5;241m.\u001b[39mRpcError \u001b[38;5;28;01mas\u001b[39;00m exc:\n\u001b[0;32m---> 78\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m exceptions\u001b[38;5;241m.\u001b[39mfrom_grpc_error(exc) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mexc\u001b[39;00m\n",
      "\u001b[0;31mResourceExhausted\u001b[0m: 429 Resource has been exhausted (e.g. check quota)."
     ]
    }
   ],
   "source": [
    "from llama_index.core import Document\n",
    "from llama_index.embeddings.openai import OpenAIEmbedding\n",
    "from llama_index.core.node_parser import SentenceSplitter\n",
    "from llama_index.core.extractors import TitleExtractor\n",
    "from llama_index.core.ingestion import IngestionPipeline\n",
    "from llama_index.vector_stores.qdrant import QdrantVectorStore\n",
    "\n",
    "import qdrant_client\n",
    "\n",
    "client = qdrant_client.QdrantClient(location=\":memory:\")\n",
    "vector_store = QdrantVectorStore(client=client, collection_name=\"demo\")\n",
    "\n",
    "pipeline = IngestionPipeline(\n",
    "    transformations=[\n",
    "        SentenceSplitter(chunk_size=25, chunk_overlap=0),\n",
    "        TitleExtractor(),\n",
    "        # OpenAIEmbedding(),\n",
    "        embedding_model3,\n",
    "    ],\n",
    "    vector_store=vector_store,\n",
    ")\n",
    "\n",
    "# Ingest directly into a vector db\n",
    "pipeline.run(documents=[Document.example()])\n",
    "\n",
    "# Create your index\n",
    "from llama_index.core import VectorStoreIndex\n",
    "\n",
    "index = VectorStoreIndex.from_vector_store(vector_store)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = index.as_retriever()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[NodeWithScore(node=TextNode(id_='a7ce82cc-7615-4661-b8ed-c7e1fbcefcb5', embedding=None, metadata={'filename': 'README.md', 'category': 'codebase', 'document_title': '\"Unlocking the Secrets of the Language Models: The Phenomenal Power of LLMs\"'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={<NodeRelationship.SOURCE: '1'>: RelatedNodeInfo(node_id='b5f9fa68-6b4b-42e5-917c-0f37e1428b66', node_type=<ObjectType.DOCUMENT: '4'>, metadata={'filename': 'README.md', 'category': 'codebase'}, hash='3183371414f6a23e9a61e11b45ec45f808b148f9973166cfed62226e3505eb05'), <NodeRelationship.PREVIOUS: '2'>: RelatedNodeInfo(node_id='42339ea9-6616-4871-930f-e4a3f264d732', node_type=<ObjectType.TEXT: '1'>, metadata={'filename': 'README.md', 'category': 'codebase'}, hash='f915a0b2115f78f5ea52d166fb73954e93e7986dd9080c0e1605b194df6a50e4'), <NodeRelationship.NEXT: '3'>: RelatedNodeInfo(node_id='2f75f4ba-53b1-4465-9a6b-0df9055f9039', node_type=<ObjectType.TEXT: '1'>, metadata={}, hash='22b5c3a2cb920659a2e615b0c028cd1f11f729b7a2df4430f78dc3467f8f5a61')}, text='in 5 lines of code. Our lower-level APIs allow advanced users', mimetype='text/plain', start_char_idx=1089, end_char_idx=1150, text_template='[Excerpt from document]\\n{metadata_str}\\nExcerpt:\\n-----\\n{content}\\n-----\\n', metadata_template='{key}: {value}', metadata_seperator='\\n'), score=0.1585200683900692),\n",
       " NodeWithScore(node=TextNode(id_='fcce7726-0bfb-4a84-b034-c6b3682681e1', embedding=None, metadata={'filename': 'README.md', 'category': 'codebase', 'document_title': '\"Unlocking the Secrets of the Language Models: The Phenomenal Power of LLMs\"'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={<NodeRelationship.SOURCE: '1'>: RelatedNodeInfo(node_id='b5f9fa68-6b4b-42e5-917c-0f37e1428b66', node_type=<ObjectType.DOCUMENT: '4'>, metadata={'filename': 'README.md', 'category': 'codebase'}, hash='3183371414f6a23e9a61e11b45ec45f808b148f9973166cfed62226e3505eb05'), <NodeRelationship.PREVIOUS: '2'>: RelatedNodeInfo(node_id='fed39a4c-511b-4a8d-9348-b69c44840807', node_type=<ObjectType.TEXT: '1'>, metadata={'filename': 'README.md', 'category': 'codebase'}, hash='a4a9f9faa99154cfcff1404aa6dda607a6ea1b7ac4bac901e54734b25ba3f3ce'), <NodeRelationship.NEXT: '3'>: RelatedNodeInfo(node_id='ece13965-3dd9-4551-9a1a-d8ac6a34a2aa', node_type=<ObjectType.TEXT: '1'>, metadata={}, hash='3fc06dafa5254cb281a5c1a2eae085df9d3ec314bed7944bd2ce2458e243998f')}, text='prompt, get back retrieved context and knowledge-augmented output.', mimetype='text/plain', start_char_idx=737, end_char_idx=803, text_template='[Excerpt from document]\\n{metadata_str}\\nExcerpt:\\n-----\\n{content}\\n-----\\n', metadata_template='{key}: {value}', metadata_seperator='\\n'), score=0.1549985116321217)]"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a.retrieve(str_or_query_bundle=\"hello\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
